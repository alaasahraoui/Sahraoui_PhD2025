\chapter{Concepts for Comparative Testing of 3D Hand Gestures in Multiple Contexts of Use}
\label{chap:Concepts}

%%%%To DO%%%%%
%Different stages of a gesture performance.
Developing multi-context comparative testing requires a thorough understanding of the subject matter. Important concepts related to the subject matter must be defined and explained in detail to achieve this. Moreover, it requires a specific analysis of state-of-the-art works, as it was accomplished in chapter \ref{chap:Related}, to gain valuable insights and knowledge.
This chapter examines the results of the analysis to identify the main principles for creating an effective comparative testing systematic procedure for hand gesture recognizers. It also describes a model that outlines the main stages for single gesture recognition testing, including Gesture Elicitation Study (GES), data acquisition, training, and testing. This approach underscores the importance of structuring the multi-context gesture recognition approach with well-defined components. At the end of the chapter, we describe the main elements of comparative testing, and we denote the systematic nature of the procedure, ensuring a seamless and efficient procedure that enhances the overall reliability and quality of the comparative testing.

\section{Terminology}
We  define a set of fundamental terms as follows:
\paragraph{Sensor:}is a device that converts a physical signal into an electrical signal that can be processed by a system~\cite{Wilson:2009}. 
\paragraph{Device:}is a physical object that enables interaction between a user and a system. A device can be an input or output device~\cite{Aquino:2010}.
\paragraph{Input Device:}is a device that provides human actions (outputs) to a machine controlled or manipulated by a user.
\paragraph{Motion Input Device:}is a device that includes at least a motion sensor and transfers raw motion data from a sensor to a system.
\paragraph{Device Family:}is a set of input devices that share the same values for a predefined set of properties, such as:
\begin{itemize}
    \item \textit{Sensing}: is the ability of the device to sense a type of signal input sent to the system.
    \item \textit{Processing}: is the ability of the system to execute different stages and methods to transform the input raw signal from the device into a digital signal.
    \item \textit{Configurability}: is the ability of the device to be configured by user input    
    \item \textit{Programmability}: is the ability of the device to accept a new set of instructions that change how it operates.
\end{itemize}
\paragraph{Input Device Category:} is a collection of different device families grouped according to one or more common criteria defined for this purpose, which differ from the device family criteria.

\section{Definitions}
\label{sec:Definitions}
    First, we define the prefixes used in this terminology:
            \begin{itemize}
                \setlength\itemsep{0em}
                \item \begin{it}Intra-\end{it}~means \enquote{within} and indicates that the elements used in gesture recognition belong to the same class.
                 \item \begin{it}Inter-\end{it}~means \enquote{between} and indicates that the elements used in gesture recognition belong to different classes.
                 \item \begin{it}Cross-\end{it}~means \enquote{extending over} and indicates that the elements used in gesture recognition combine elements of different classes.
                 \item \begin{it}Trans-\end{it}~means \enquote{across} and indicates that the elements used in gesture recognition belong to different classes for each stage as defined above.
                 \item \begin{it}Uni-\end{it}~means \enquote{One} and indicates that the elements used in gesture recognition belong to a single class.
                 \item \begin{it}Multi-\end{it}~means \enquote{Multiple} and indicates that the elements used in gesture recognition belong to multiple classes.
            \end{itemize}
    
    
    The following definitions refer to the terms that indicate the configurations of the devices used during the gesture recognition process and are illustrated in Figure~\ref{fig:classification_multidevice}. In the context of this thesis, the gesture recognition process includes two stages common to all machine learning processes: training and testing, which were defined above ~\cite{Kotsiantis:2006}. 

  \begin{itemize}
            \item \begin{bf}Inter-input device category\end{bf}~ means that the devices used in the gesture recognition process belong to different input device categories. Many examples follow this configuration.
            %~\cite{Hoffman:2010,Marin:2014}.
             \item \begin{bf}Intra-input device category\end{bf}~ means that the devices used in the gesture recognition process belong to the same input device category.
             %~\cite{Liu:2009},.
             \item \begin{bf}Cross-input device category\end{bf}~ means that the devices used in the gesture recognition process are a combination of two devices from different input device categories for each stage. 
             %Many examples follow this configuration, in ~\cite{Hoffman:2010,Marin:2014}.
             \item \begin{bf}Trans-input device category\end{bf}~ means that the devices used in the gesture recognition process belong to different input device categories for each stage.
             \item \begin{bf}Uni-Category\end{bf}~ means that the devices used in the gesture recognition process belong to one input device category.
             \item \begin{bf}Multi-Category\end{bf}~ means that the devices used in the gesture recognition process belong to multiple input device categories. 
        \end{itemize}

        \begin{itemize}
            \item \begin{bf}Inter-input device family\end{bf}~ means that the devices used in the gesture recognition process belong to the same input device category but to different input device families. 
            \item \begin{bf}Intra-input device family\end{bf}~ means that the devices used in the gesture recognition process belong to the same input device category and the same input device family. 
            \item \begin{bf}Cross-input device family\end{bf}~ means that the devices used in the gesture recognition process are a combination of two devices from the same input device category but from different input device families for each stage. 
            \item   \begin{bf}Trans-input device family\end{bf}~ means that the devices used in the gesture recognition process belong to the same input device category but to different input device families for each stage.
            \item \begin{bf}Uni-Family\end{bf}~ means that the devices used in the gesture recognition process belong to one input device category and to the same input device family. 
            \item \begin{bf}Multi-Family\end{bf}~ means that the devices used in the gesture recognition process belong to one input device category and to the different input device families. 
        \end{itemize}
    
        \begin{itemize}
            \item \begin{bf}Inter-device\end{bf}~ means that the devices used in the gesture recognition process are different but belong to the same input device category and input device family. 
            \item \begin{bf}Intra-device\end{bf}~ means that the devices used in the gesture recognition process are exactly the same and belong to the same category and family of input devices. 
            \item \begin{bf}Cross-device\end{bf}~ means that the devices used in the gesture recognition process are a combination of two devices from the same category and family of input devices for each stage. 
            \item \begin{bf}Trans-device\end{bf}~ means that different devices are used in the gesture recognition process, but belong to the same category and family of input devices for each stage. 
            \item \begin{bf}Uni-device\end{bf}~ means that there is only one device used in the gesture recognition process.
            \item \begin{bf}Multi-device\end{bf}~means that the devices used in the gesture recognition process are different~\cite{Aquino:2010}.
        \end{itemize}
\newpage
\begin{sidewaysfigure}[!h]
    \centering
    %\hspace{-140px}
    \includegraphics[height=0.59\textheight]{Figures/Chap3/Chap3_FigClassification.pdf}
    \caption{Taxonomy of configurations for input devices used in the gesture recognition process.}
    \label{fig:classification_multidevice}
\end{sidewaysfigure}

\clearpage



Based on the classification described above, we can provide an example that expands Buxton's taxonomy of input devices~\cite{Buxton:1983} by including the terminology defined in this chapter. We will introduce two examples. The first example defines two criteria for the category. As a result, it uses a motion mechanical device for training and a motion sensing device for testing. This describes an inter-device category relation for the recognition process. The second example shown is using a ring zero for training and the Wii Remote for testing, which represents a trans-device family relation. These examples are shown in the following extended Buxton's taxonomy in Figure~\ref{fig:buxton_taxonomy}.
%To Edit
\begin{figure}[h!]
    \centering
    \hspace{40px}
    \includegraphics[width=\textwidth,trim=5 50 0 0 ]{Figures/Chap3/Buxton_taxonomy_Extension.pdf}
    \vspace{15px}
    \caption{Taxonomy of input devices from ~\cite{Buxton:1983} extended.}
    \label{fig:buxton_taxonomy}
\end{figure}
\vspace{-15px}
\section[Temporal Relationships in the \enquote{Cross} Configuration]{\large Temporal Relationships in the \enquote{Cross} Configuration}
The aforementioned \enquote{cross} configurations require the use of two different devices at different stages of the gesture recognition process. The recording of the various possible relationships described by Allen's interval algebra are grouped into two relational properties: the sequentiality and the simultaneity of the gesture as it is captured by the devices as it is shown in Table ~\ref{Tab:Allen_interval_CrossConf}, where $G_{DevA}$ and $G_{DevB}$ represent the gesture recorded by device A and device B, respectively.

\begin{table}[!h]
 \caption{Allen's interval algebra relations in \enquote{cross-device} configuration.}
    \label{Tab:Allen_interval_CrossConf}
\hspace{0 cm}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|>{\centering\arraybackslash}m{.2975\linewidth}|>{\centering\arraybackslash}m{.17\linewidth}|>{\centering\arraybackslash}m{.75\linewidth}|}
\hline
\multicolumn{1}{|c|}{Allen Statements} & \multicolumn{1}{|c|}{Visual  Example} & Chronological Sequence \\ \hline
$G_{DevA}$ before $G_{DevB}$  &   \includegraphics[scale=0.7,trim=0 0 0 -5]{Figures/Chap3/Allen_Interval/Before.pdf}          &        \smaller $Start(G_{DevA})~<~ End(G_{DevA})~<~Start(G_{DevB})~<~End(G_{DevB})$                \\ \hline
$G_{DevA}$ equals $G_{DevB}$  &     \includegraphics[scale=0.7,trim=0 0 0 -5]{Figures/Chap3/Allen_Interval/Equals.pdf}                   &        \smaller $Start(G_{DevA})~=~ Start(G_{DevB})~<~End(G_{DevA})~=~End(G_{DevB})$                 \\ \hline
$G_{DevA}$ meets $G_{DevB}$   &      \includegraphics[scale=0.7,trim=0 0 0 -5]{Figures/Chap3/Allen_Interval/Meets.pdf}                  &    \smaller $Start(G_{DevA})~<~ End(G_{DevA})~=~Start(G_{DevB})~<~End(G_{DevB})$                        \\ \hline
$G_{DevA}$ overlaps $G_{DevB}$ &\includegraphics[scale=0.7,trim=0 0 0 -5]{Figures/Chap3/Allen_Interval/Overlaps.pdf}        &         \smaller $Start(G_{DevA})~<~ Start(G_{DevB})~<~End(G_{DevA})~<~End(G_{DevB})$                    \\ \hline
$G_{DevA}$ during $G_{DevB}$ &\includegraphics[scale=0.7,trim=0 0 0 -5]{Figures/Chap3/Allen_Interval/During.pdf}       &              \smaller $Start(G_{DevA})~<~ Start(G_{DevB})~<~End(G_{DevB})~<~End(G_{DevA})$           \\ \hline
$G_{DevA}$ starts $G_{DevB}$&           \includegraphics[trim=0 0 0 -5,scale=0.7]{Figures/Chap3/Allen_Interval/Starts.pdf}            &        \smaller $Start(G_{DevA})~=~ Start(G_{DevB})~<~End(G_{DevA})~<~End(G_{DevB})$                 \\ \hline
$G_{DevA}$ finishes $G_{DevB}$&        \includegraphics[trim=0 0 0 -5,scale=0.7]{Figures/Chap3/Allen_Interval/Finishes.pdf}                &         \smaller $Start(G_{DevB})~<~ Start(G_{DevA})~<~End(G_{DevA})~=~End(G_{DevB})$                \\ \hline
\end{tabular}
}
\end{table}

\section[Procedure for Single Gesture Recognition Testing]{\smaller\selectfont{Procedure for Single Gesture Recognition Testing}}
\label{sec:Testing_Procedure}
The gesture recognition testing procedure involves several stages: elicitation, data acquisition, training, and testing. These stages are commonly used in the literature for gesture recognition evaluation. However, some literature works include certain stages, while others are limited to others~\cite{Laviola:2013,Kotsiantis:2006,Yasen:2019}.  During the Gesture Elicitation Study stage, participants collaborate to create gestures for specific tasks that reflect the end user's preferences, as briefly mentioned in subsection ~\ref{subsec:Gesture_Taxonomy}. 

The data acquisition stage involves capturing gesture data using various devices. The training stage is dedicated to developing the recognizer model, depending on the chosen classification technique. Finally, the test stage evaluates the performance of the trained model by measuring its ability to recognize new gestures. Similarly to  ASPLe work~\cite{Abbas:2020} that provides systematic process support for the development of self-adaptive software systems with reuse. 

We use the Software \& Systems Process Engineering Meta-Model (SPEM) notation \cite{OMG:2008} (Figure~\ref{fig:SPEM_Definition})  to illustrate the gesture recognition testing procedure. 

As a specification of a Unified Modeling Language (UML) metamodel provided by the Object Management Group (OMG), SPEM is used to represent a wide range of software development processes and their components. Essentially, it functions as an \enquote{ontology} for software development processes, that provides a minimum set of process modeling elements that are applicable to any software development process. It does not impose specific models or constraints for particular fields such as software engineering, requirements engineering, or project management.


\begin{figure}[h!]
    \hspace{-5px}
    \includegraphics[width=\textwidth]{Figures/Chap3/Testing_Procedure_SPEM_Definition.pdf}
    \caption{ A subset of the SPEM notation relevant to this work.}
    \label{fig:SPEM_Definition}
\end{figure}
\vspace{-8pt}
\begin{itemize}
    \item "Work Definition" refers to the specific type of operation that describes the work executed in a process.
    \item  "Work Product" denotes any tangible information either produced, consumed, or altered by a process.
    \item  "Process Role" outlines the responsibilities related to specific Work Products.
    \item  "Activity" signifies a task performed by a single Process Role.
    \item "Document" is a specific category (a stereotype) of Work Product.
    \item "Guidelines" provides detailed information regarding a resource.
   
\end{itemize}

    The roles considered in this procedure are related to experimental design~\cite{Chin:2001}. The \textit{Researcher/Experimenter} is responsible for creating and testing the gesture interaction system. Their goal is to obtain accurate results that either support or challenge a given theory about the product they've designed and developed. This includes conducting the Gesture Elicitation Study, acquiring data when necessary, and carrying out the training and testing of the gesture recognizer(s). 
    
    The participant role is responsible for providing data and feedback throughout the Gesture Elicitation Study and the Acquisition data stages, both by helping with collaboratively constructing the gesture set and providing and validating gestures to the recognizer(s) being tested.

    The procedure includes four disciplines corresponding to the previously mentioned stages. This procedure is designed to support the different stages of hand gesture recognition evaluation. In the following sections, each stage and its interactions are described in detail as a workflow.

    \section{Workflow}
    The workflow that links all the disciplines together is shown in Figure~\ref{fig:procedure_Workflow}. All \textit{Work Products}, \textit{Documents}, and \textit{Guidelines} are also shown as either resources (if they are used in the discipline) or outcomes  (if they are produced). We indicate only the most logical or frequently used shortcuts on the diagrams, rather than listing all possible shortcuts. The workflow shows that we can initiate the procedure from three of the four stages, namely: Gesture Elicitation Study, Data Acquisition, and Training. It's important to note that we can iterate on each stage and progress through them sequentially, the whole workflow forming a loop.
    
    

\begin{figure}[h!]
    \hspace{-5px}
    \includegraphics[width=\textwidth]{Figures/Chap3/Testing_Procedure_Workflow.pdf}
    \caption{Gesture recognition testing procedure workflow.}
    \label{fig:procedure_Workflow}
\end{figure}
\subsection{Stage1: Gesture Elicitation Study}
Gesture Elicitation Studies (GESs) are commonly used methods in HCI to design intuitive gesture commands for controlling interactive devices, applications, and systems~\cite{KÃ¼hnel:2011,Ruiz:2011,Vanderdonckt:2019,Wobbrock:2009,Zaiti:2015}. These studies cover different contexts of use~\cite{VillarrealNavarez:2020}, spanning from classical devices to more modern ones~\cite{Villarreal:2022,Villarreal:2022:Squeeze}, and are thoroughly studied in the literature (see~\cite{Villarreal:2024} for a systematic literature review).

According to~\citet{Wobbrock:2009,Magrofuoco:2019,Villarreal:2024}, the GES workflow is structured in several steps:
\begin{enumerate}
    \item Define a study by determining the parameters and specifications for a GES. This includes defining the task set, referents, context of use, and data collection.
    \item Conduct the study by conducting the GES according to the defined parameters. The result is a collection of elicited gestures and associated data.
    \item Classify the gestures by categorizing them using a method such as clustering criteria. This organizes similar gestures together.
    \item Measure the gestures by calculating various measurements using a measurement method. These measurements provide insight into the characteristics and properties of the gestures. 
    \item Discuss the gestures by interacting with stakeholders to reach a consensus. This ensures that the selected gestures are suitable for the intended use.
    \item Export the gestures by converting the consensus data into a format compatible with gesture recognizers or modules that support gesture recognition. This enables effective use in practical applications.
\end{enumerate}
\paragraph{Workflow}
The workflow in Figure~\ref{fig:GES_Workflow} begins with the pre-test phase. In this phase, the \textit{Researcher/Experimente}r defines the structure of the study, following the GES Guidelines. After this, the study is presented to the participant, who then completes questionnaires and undergoes tests. The \textit{Researcher/Experimenter} then introduces the study apparatus, including the input device, and presents an example and the different referents for the study. For each referent, participants suggest gestures using the device, which are observed and recorded. Following the gesture proposal, participants provide feedback based on their experience. If unsatisfied, they can suggest alternative gestures. Afterward, they will take a post-test quiz in the post-test phase. The \textit{Researcher/Experimenter} then classifies the gestures, forms a consensus gesture set, and consolidates the results using this set as a reference.
\begin{figure}[ht!]
    \hspace{-5px}
    \includegraphics[width=\textwidth]{Figures/Chap3/Testing_Procedure_GES_WF.pdf}
    \caption{Activity diagram for Gesture Elicitation Study.}
    \label{fig:GES_Workflow}
\end{figure}
\subsection{Stage2: Data Acquisition}
The stage of gesture acquisition is critical for evaluating gesture recognizers during the training and testing stages. Evaluators can develop effective methods by carefully collecting and analyzing datasets of gestures and choosing one or more appropriate input devices to capture gesture data~\cite{Cheema:2013,Liu:2009,Yasen:2019}, such as electromyography (EMG) armbands, Time-of-Flight cameras, and inertial measurement units. Additionally, video recording is essential when collecting gestures, whatever the device used. It enables outliers to be detected and variances to be understood, improving data quality and reliability. Furthermore, they should consider different types of gestures. As mentioned earlier, conducting a study to elicit gestures can provide a dataset of gestures that define users' preferences. Other methods, such as challenges and contests, can also be used as datasets of gestures for evaluation~\cite{Caputo:2019,DeSmedt:2017}. An evaluation dataset should ideally include at least two samples for each class of gesture, with one sample used for training and the other for testing purposes~\cite{Hoffman:2010,Liu:2009}. Participants involved in the data collection process should perform two or more samples per gesture class, and these gesture classes could be augmented with synthetic data to ensure consistency and accuracy throughout the evaluation process~\cite{Leiva:2015,Molina:2014}.
\paragraph{Workflow}
The data acquisition stage of gesture recognition involves several activities described in Figure~\ref{fig:Data_Workflow}. First, the \textit{Researcher/Experimenter} designs the study, defining its scope and objectives. The \textit{Acquisition Parameters} document is important for this activity.  Next, the \textit{Researcher/Experimenter} presents the study to the participant, explaining its purpose, design, and expected results. They then introduce the input device that will be used to capture the gestures, detailing its specifications. The \textit{Researcher/Experimenter} then presents examples of the gestures to be acquired, followed by a specific gesture class stimulus. Participants are asked to imitate these gestures using the input device. The next activities involve participants performing the gestures and providing feedback on their gestures. The \textit{Researcher/Experimenter} collects these gesture samples for further analysis.  Finally, the researcher consolidates the samples into a dataset and reviews it to ensure its integrity and usability.
\begin{figure}[h!]
    \hspace{-5px}
    \includegraphics[width=\textwidth]{Figures/Chap3/Testing_Procedure_Data_WF.pdf}
    \caption{Activity diagram for Data acquisition.}
    \label{fig:Data_Workflow}
\end{figure}
\subsection{Stage3: Training}
The training stage of gesture recognition evaluation focuses on creating a gesture model~\cite{Kotsiantis:2006,Yasen:2019}. The process consists of several steps. First, a training data set is created from the collected gesture data.   Then, a data pre-processing stage is applied to each sample of the gesture classes that compose the training set to remove noise and standardize the data. Finally, during the training stage, different training methods, such as feature extraction and normalization, are used based on the recognizer's classification technique. The training stage may have specific parameters, such as the number of templates for each class in the training set.

\paragraph{Workflow}
Developing an effective gesture recognizer involves several activities for the training stage shown in Figure\ref{fig:Training_Workflow}. Firstly, the \textit{Researcher/Experimenter} designs a training model by setting the necessary parameters relying on the \textit{Parameters} document. Following this, \textit{Create Datasets} produces three work products (\textit{Training Set, Testing Set, and Validation Set}). These are created using appropriate \textit{Scenarios} and \textit{Techniques} and may be segmented if required. Two of these datasets are then preprocessed (\textit{Preprocess Datasets}) for the training phase. Afterward, the \textit{Recognizer(s)} are trained on the \textit{Training Set} produced work. Next, to validate, a gesture template is tested for validation (\textit{Test a Gesture Template for Validation}) and the performance is evaluated on the \textit{Validation Set}. Ultimately, the overall performance of the recognizer(s) is validated, resulting in recognizer(s) that can efficiently identify gestures.
\begin{figure}[h!]
    \hspace{-5px}
    \includegraphics[width=\textwidth]{Figures/Chap3/Testing_Procedure_Training_WF.pdf}
    \caption{Activity diagram for Training.}
    \label{fig:Training_Workflow}
\end{figure}
\subsection{Stage4: Testing}
The testing stage evaluates the performance of the gesture recognizer by predicting the class label of a candidate gesture using the learned gesture model~\cite{Kotsiantis:2006,Yasen:2019}. Unlike the offline training stage, the testing stage is considered an online phase. In this case, a gesture template is selected from the gesture set, which is not part of the training set. Or it could result from segmenting a continuous stream of data using various methods (See Subsection~\ref{subsec:Gesture_Segmentation}) to extract the important segment of the gesture.
\begin{figure}[h!]
    \hspace{-5px}
    \includegraphics[width=\textwidth]{Figures/Chap3/Testing_Procedure_Testing_WF.pdf}
    \caption{Activity diagram for Testing.}
    \label{fig:Testing_Workflow}
\end{figure}
\paragraph{Workflow}
The testing discipline for 3D hand gesture recognizers is described in the Figure~\ref{fig:Testing_Workflow} and is performed by the \textit{Researcher/Experimenter}. The \textit{Design the Testing}  is an essential activity that involves setting the test parameters using the document \textit{Parameters}. This is followed by the segmentation of the \textit{Test Set} work product, an activity that is part of the Pre-testing phase. This is followed by the preprocessing of the test set, which requires the use of the \textit{Test Set} work product as both input and output. Testing itself involves various documents (\textit{Scenarios, Recognizer(s), Techniques, Parameters}) and culminates in the generation of specific work product \textit{Testing Results}. The final activity is to analyze these results to extract meaningful insights from the data.
\section{Comparative Testing}
Numerous comparative testing studies have been conducted in the field of gesture recognition. For instance, the study by Khan \etal~\cite{Khan:2012} examines various vision-based hand gesture recognition systems, focusing on segmentation, feature detection, and extraction phases.

Another study assesses the effectiveness of a proposed feature selector method, comparing it with three other methods based on four performance measures and prediction time. The aim is to understand the impact of the feature selector on the performance of data fusion in activity recognition~\cite{Wang:2016}.

Ferrer \etal~\cite{Ferrer:2011} conducted a comparative study on various features for predicting human motion based on minimum curvature variance.

Popov \etal~\cite{Popov:2022} Compared the performance of the developed two-stage real-time hand image-based gesture recognition system in complex backgrounds with MobileNets, a state-of-the-art neural network for real-time mobile vision applications, one of the concerns expressed in their work is the lack of comparative testing methods.

A different study compares various devices within a game design context~\cite{Khalaf:2019}. The work done by Erazo \etal~\cite{Erazo:2020} who compared five 2D stroke gesture recognition algorithms on multiple datasets, described briefly the experimentation procedure, which is an instantiation of the procedure described in Section ~\ref{sec:Testing_Procedure}.
Despite numerous comparative studies in the field of gesture recognition, many of them were carried out opportunistically or were ad hoc methods~\cite{ReddyAnitha:2022,Sluyters:2023b,Zheng:2022}, none have focused on defining a systematic procedure for 3D hand gesture recognition using template-based recognizers.
\subsection{Artefacts for Comparative Testing}\label{subsec:Artefacts}
The goal of the gesture recognition comparative testing method is to assess the performance of various template-based gesture recognizers in multiple contexts of use. These evaluations usually involve comparing the accuracy or recognition rate of the recognizers and considering factors such as computational complexity, real-time capability, and feasibility on affordable hardware. Comparative testing is conducted by applying a set of predefined test cases to determine the most accurate gesture recognizer. It relies on a framework to compare different template-based gesture recognizers. From the literature, we extracted the key components of comparative testing for gesture recognition.



% Figure~\ref{fig:comparaive_scheme} shows the main components of the comparative testing framework.

\subsection{Recognizers}
The recognizers serve as the central component of the comparative testing framework. The evaluator includes a collection of template-based recognizers that are capable of identifying a candidate gesture that matches a gesture class defined by a limited number of training templates. These recognizers should be implemented in the same programming language and be compatible with datasets consisting of 3D hand gestures.

\subsection{Techniques}
There are several techniques used to evaluate the performance of gesture recognizers~\cite{Anthony:2010,Anthony:2012,Kotsiantis:2006,Wobbrock:2007,Vatavu:2018,Vatavu:2012b}. One common technique is K-fold cross-validation, where a dataset is divided into k groups and each group is used as a test set during the evaluation process. Other techniques described in ~\cite{Berenguer:2019} are the 50\%-50\% technique, where the dataset is divided into equal training and test sets. The leave-one-out cross-subject evaluation trains models on data from all but one subject and tests them on the subject not included in the training set. The leave-one-out cross-session evaluation focuses on hand gestures recorded from a single subject and tests the models in the remaining session after training on data from all other sessions.
\subsection{Scenarios}
The procedure of comparative testing aims to handle the 3D hand gesture recognizer in multiple contexts. The number of devices used and the different configurations during the gesture recognition process determine the system's ability to handle both single-device and multi-device configurations. As explained in Section ~\ref{sec:Definitions}, for instance, in the Cross-input Device family scenario, the recognizer is trained using samples performed from two or more devices of the same device family and category and then tested using the same device.

When studying the user variable, two commonly used scenarios inspired by the k-fold cross-validation technique are user-dependent and user-independent.

Unlike the cross-validation technique, these techniques randomly select training and test samples from the dataset, without dividing it into k groups. In the user-dependent scenario, the evaluation involves training and testing a recognizer with gestures produced by the same end user. Randomly selected samples per gesture class are used for training, and one remaining sample per gesture class is used for testing. In the user-independent scenario, the recognizers are trained with gestures produced by a set of independent participants who are randomly selected to train the recognizer. The recognizer is tested with gestures produced by an independent end user. For each participant, one sample per gesture class is randomly selected for testing.
\subsection{Datasets}
The datasets used to train and test the recognizers can consist of various types of gestures performed by one or multiple users. The selection of datasets depends on factors such as the context of use, the diversity and variability of gestures, and the availability of reference data. It is advisable to use publicly available datasets that allow for reproducibility.

The evaluator can choose to include one or more datasets from one or more devices. These datasets may differ in terms of gesture classes, participants, and equipment and can impact the configuration and scenario of the comparative testing.
\subsection{Parameters}
Comparative testing requires the configuration of numerous parameters within the testing procedure. These parameters are context-dependent and vary based on the test scenario.

In the user-dependent scenario, the recognizer is trained and tested using gestures produced by the same end user. The main variables in this scenario are the number of templates (T) and the number of tests (repetitions) (R).

In the user-independent scenario, the recognizer is trained with gestures produced by one or more end users and tested with gestures produced by an independent end user. This scenario includes an additional variable, the number of participants (P).

We can adjust the parameter values by assigning them any value. These values can be derived from experience, previous studies, or constraints from the dataset. The number of additional parameters may increase depending on the devices used during the gesture recognition procedure, especially when considering the different scenarios brought about by the use of multiple devices.
\subsection{Procedure for Systematic Comparative Testing}
Based on the detailed testing procedure outlined in Section~\ref{sec:Testing_Procedure}, it was determined that the single gesture recognition testing procedure shares artifacts with comparative testing, as detailed in Subsection~\ref{subsec:Artefacts}. The introduction of a unified process for single gesture recognition testing into the workflow, along with the capability to repeat each stage multiple times, offers a systematic approach to this procedure through clearly defined activities. The primary function of the comparative testing procedure is to test the recognizers on a set of predefined test cases, which utilize the same components as the single testing procedure and the same activities. The reuse of a coherent set of activities highlights its completeness and underscores the systematic nature of comparative testing. For example, if we aim to compare two recognition algorithms across two experiments and use different procedures for each, the results won't be comparable. Conversely, by following the stages of the systematic procedure, we can ensure the results are comparable.
\section{Conclusion}
In conclusion, this chapter provides an overview of the definitions and concepts involved in the comparative testing of gesture recognizers for multiple contexts of use. It emphasizes the importance of a thorough understanding of concepts and well-defined testing procedures to ensure the development of robust and accurate gesture recognizers in multiple contexts of use.
In the upcoming chapters, this procedure will be applied using specific examples, with a focus on two different scenarios: comparing template-based gesture recognizers on 3D hand unipath dynamic gestures and template-based gesture recognizers on 3D hand \enquote{multipath dynamic} gestures.

% \begin{figure}[h!]
%     \hspace{-90px}
%     \includegraphics[scale=.5]{Figures/Chap3/Evaluation_Scheme.pdf}
%     \caption{Diagram of possible evaluation procedures.}
%     \label{fig:Evaluation_scheme}
% \end{figure}
% \newpage



% \section{Gesture Customization}
% Rapid prototyping gesture recognition require using a small set of gestures to comply with the required performance. In addition, user-defined gesture sets are suitable for rapid prototyping; in particular, such gestures have many advantages, including achieving high performance with a small dataset and being well adapted to users compared to pre-designed gestures, as described in ~\cite{Nacenta:2013,Taranta:2021}. Although unified gesture sets also have their advantages, especially in a collaborative context and the transferability of gestures across applications, they are not always easy to use. Therefore, we consider a compromise between the two, relying for the mid-term evaluation on elicited gestures and considering customization as an option.

% \section{Clifford's Algebra}
% In the following subsections we will introduce some principal notions that we will use in the next chapters of this thesis that rely on  Clifford's Algebra and their 3D adaptations. 
% \subsection{Basic Gesture}
% For 2D gesture recognition, and particularly in !FTL ~\cite{Vanderdonckt:2018}, a basic gesture is introduced as a pair of vectors $(\ora{u},\ora{v})$ that connect a set of 3 consecutive points A, B, and C. In Figure~\ref{fig:basic_gesture}, the pair of vectors (a) illustrates a basic gesture $(\ora{u},\ora{v})$ that formed by 3 points and their vectors $\ora{u}=\ora{AB}$ and $\ora{v}=\ora{BC}$ in a 3D space.

% \begin{figure}[h!]
%     \centering
%     \includegraphics[scale=.6]{Figures/Chap3/Basic_Shape.pdf}
%     \caption{A representation of a \enquote{basic gesture} (a) $(\ora{u},\ora{v})$ and a \enquote{shape} (b) \textit{s}$(\ora{u},\ora{v})$.}
%     \label{fig:basic_gesture}
% \end{figure}

% \subsection{Shape}\label{sec:lester_shape}
% Lester ~\cite{Lester:1996} defined a complex number to characterize the shape of triangle which can be used to measure the similarity. Luzzi ~\cite{Luzzi:2018} consider a plane shape of a basic gesture as a triangle whose vertice are the 3 points of that basic gesture. Figure~\ref{fig:basic_gesture} (b) highlights the shape $s(\ora{u},\ora{v})$ built on top of a basic gesture $(\ora{u},\ora{v})$ with vectors $\ora{u}=(u_1,u_2) \in \mathbb{R}_2$ and $\ora{v}=(v_1,V_2) \in \mathbb{R}_2$. Using Clifford's algebra, the Lester's plane shape becomes a pair of coordinates that describes the shape of the basic gesture, see equation~\ref{math:lester_shape}.
% \bigbreak
% \begin{gather}
%     s(\ora{u},\ora{v})\\
%     = \ora{u}(\ora{v})^{-1}\\
%     = \ora{u}\cdot(\ora{v})^{-1} + \ora{u}\land(\ora{v})^{-1}\\
%     = \frac{u_1v_1+u_2v_2}{(v_1)^2+(v_2)^2} + \frac{1}{(v_1)^2+(v_2)^2} \det\begin{pmatrix} u_1 & u_2 \\ v_1 & v_2 \end{pmatrix}\\
%     = \left( \frac{u_1v_1+u_2v_2}{(v_1)^2+(v_2)^2} , \frac{u_1v_2-u_2v_1}{(v_1)^2+(v_2)^2} \right) \in \mathbb{R}_2 \label{math:lester_shape}
% \end{gather}

% \subsection{Local Shape Distance}\label{sec:lsd}
% Given 2 shapes $s(\ora{a},\ora{b})=(\alpha_1,\alpha_2)$ and $s(\ora{u},\ora{v})=(\beta_1,\beta_2)$, the local shape distance corresponds simply to the Euclidean distance between both shapes.
% \begin{gather}
%     LSD(\ora{a},\ora{b},\ora{u},\ora{v})\\
%     = LSD(s(\ora{a},\ora{b}), s(\ora{u},\ora{v}))\\
%     = \sqrt{(\beta_1-\alpha_1)^2+(\beta_2-\alpha_2)^2}
% \end{gather}